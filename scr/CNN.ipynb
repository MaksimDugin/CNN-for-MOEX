{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python 3.6.13\n",
    "\n",
    "tensorflow  1.2.0\n",
    "\n",
    "Ubuntu 16.04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip freeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataReaderRL:\n",
    "\n",
    "    def __init__ ( self ):\n",
    "        self.a = 0\n",
    "\n",
    "\n",
    "    def get_filelist ( self, rootpath ):\n",
    "\n",
    "        pathlist = list()\n",
    "\n",
    "        country = os.listdir( rootpath )\n",
    "        for i in range(  len(country) ):\n",
    "            country[i] = rootpath + str( country[i] ) + '/'\n",
    "\n",
    "        datelist = list()\n",
    "        for i in range( len(country) ):\n",
    "            datelist = os.listdir( country[i] ) \n",
    "\n",
    "            for j in range( len (datelist) ):\n",
    "                pathlist.append( country[i] + datelist[j] + '/')\n",
    "\n",
    "\n",
    "        pathlist.sort()\n",
    "\n",
    "        #for i in range( len(pathlist) ):\n",
    "         #   print pathlist[i]\n",
    "        print('numof all data : ', len( pathlist ))\n",
    "        return pathlist\n",
    "\n",
    " \n",
    "    def readRaw_generate_X  (self, filepath, height, width ):\n",
    "\n",
    "        # Generate height by wdith   input chart image\n",
    "        \n",
    "\n",
    "        f       =   open ( filepath, 'r' )\n",
    "        rawdata =   f.read()\n",
    "        rawdata =   rawdata.split( '\\nF\\n'  )\n",
    "        DataX   =   list()\n",
    "        N       =   len( rawdata ) - 1\n",
    "        Days    =   len( rawdata[0].split( '\\nE\\n' ) )\n",
    "        print(Days, N)\n",
    "\n",
    "        for c in range( N ) :\n",
    "            state_seq  = rawdata[c].split( '\\nE\\n' )\n",
    "\n",
    "            # matrix seq for company c\n",
    "            matrix_seq = list()\n",
    "            for t in range ( Days ):\n",
    "                matrix  = np.zeros( ( height, width ) )\n",
    "                rows    = state_seq[t].split('\\n')\n",
    "                #print(Days, t)\n",
    "                # input matrix on day t\n",
    "                for r in range ( height ):\n",
    "                    row  = rows[r].split( ' ' )     \n",
    "                    for w in range( width ):\n",
    "                        matrix[r][w] = int( row[w] )\n",
    "\n",
    "                matrix_seq.append( matrix )\n",
    "\n",
    "            DataX.append ( matrix_seq )\n",
    "\n",
    "\n",
    "        return DataX \n",
    "                                          \n",
    "    def readRaw_generate_Y    ( self, filepath, N, Days ):\n",
    "\n",
    "        # Generate input price change L_c^t\n",
    "        f       =   open ( filepath, 'r' )\n",
    "        rawdata =   f.read()\n",
    "        rawdata =   rawdata.split( '\\n'  )\n",
    "        DataY   = list()\n",
    "\n",
    "        if  (len(rawdata) - 1) != (N*Days) :\n",
    "            print('number of input data is invalid')\n",
    "\n",
    "        cnt     = 0\n",
    "        for c in range ( N ) :\n",
    "            return_seq = list()\n",
    "\n",
    "            for t in range(Days):\n",
    "                return_seq.append( float( rawdata [cnt] ) )\n",
    "                cnt = cnt + 1\n",
    "\n",
    "            DataY.append ( return_seq )\n",
    "\n",
    " \n",
    "        return DataY "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConstructCNN:\n",
    "\n",
    "    def __init__        ( self, Height, Width, FSize, PSize, PStride, NumAction ):\n",
    "\n",
    "        self.H          = Height\n",
    "        self.W          = Width\n",
    "        self.FSize      = FSize\n",
    "        self.PSize      = PSize\n",
    "        self.PStride    = PStride\n",
    "        self.NumAction  = NumAction\n",
    "    \n",
    "\n",
    "\n",
    "    def QValue   ( self, state, isTrain ):\n",
    "\n",
    "\n",
    "        X           = tf.reshape            ( state, [ -1, self.H, self.W, 1] )\n",
    "        M,V         = tf.nn.moments         ( X, [0,1,2,3] )\n",
    "        X           = self.normalize_input  ( X, M, V )\n",
    "\n",
    "        # CNN Layer\n",
    "        Layer1, M1,V1    = self.stackedLayer ( 'L1', X,      self.FSize, self.PSize, self.PStride,  1,   16,  2, isTrain )\n",
    "        Layer2, M2,V2    = self.stackedLayer ( 'L2', Layer1, self.FSize, self.PSize, self.PStride,  16,  32,  2, isTrain )\n",
    " \n",
    "        # Fully Connected Network\n",
    "        # L6    :  Batch x inputSize\n",
    "        L6          = tf.contrib.layers.flatten( Layer2 )\n",
    "        FC1         = self.FCLayer      ( \"FC_1\", L6, int( L6.get_shape()[-1]), 32, isTrain )\n",
    "        FC2         = self.FinalLayer   ( \"FC_2\", FC1, 32, self.NumAction, isTrain )\n",
    "\n",
    "        # FC2 : Batch * 3\n",
    "        rho         = FC2\n",
    "        eta         = tf.one_hot(  tf.argmax( rho, 1 ), self.NumAction, on_value = 1, off_value = 0, dtype = tf.int32 )\n",
    "\n",
    "        return rho, eta \n",
    "\n",
    "\n",
    "    def optimize_Q          ( self, Q, A, Y, batchsize, learning_rate ):\n",
    "\n",
    "        # Q : Batch * numaction\n",
    "        # A : Batch * numaction \n",
    "\n",
    "        # update BatchNorm var first, and then update Loss, Opt\n",
    "        updates     = tf.get_collection ( tf.GraphKeys.UPDATE_OPS )\n",
    "        trablevars  = tf.trainable_variables()\n",
    "\n",
    "        with tf.control_dependencies ( updates ):\n",
    "\n",
    "            Loss    = tf.reduce_sum             ( tf.square(  Y - (Q*A) ) ) /   batchsize \n",
    "            opt     = tf.train.AdamOptimizer    ( learning_rate )\n",
    "\n",
    "            grads   = opt.compute_gradients     ( Loss )\n",
    "            minz    = opt.minimize              ( Loss )\n",
    "        \n",
    "        return  Loss, grads, updates, trablevars, minz \n",
    "\n",
    "\n",
    "    def FinalLayer             ( self, Name, Lin, inputSize, LayerSize, isTrain ):\n",
    "        with tf.variable_scope(Name, reuse=True  ):\n",
    "\n",
    "            # inputSize, LayerSize\n",
    "            W   = tf.get_variable( Name, [ inputSize, LayerSize],  initializer = tf.contrib.layers.xavier_initializer()  )\n",
    "            B   = tf.get_variable( Name + \"_B\" ,  initializer = tf.truncated_normal( [1,LayerSize],stddev = 0.01) )\n",
    "            Out = tf.matmul (Lin, W) + B\n",
    "            return Out\n",
    "\n",
    "\n",
    "    def FCLayer             ( self, Name, Lin, inputSize, LayerSize, isTrain ):\n",
    "        with tf.variable_scope(Name, reuse=True  ):\n",
    "\n",
    "            # inputSize, LayerSize\n",
    "            W   = tf.get_variable( Name, [ inputSize, LayerSize],  initializer = tf.contrib.layers.xavier_initializer()  )\n",
    "            B   = tf.get_variable( Name + \"_B\" ,  initializer = tf.truncated_normal( [1,LayerSize],stddev = 0.01) )\n",
    "\n",
    "            Out = tf.matmul (Lin, W) + B\n",
    "            BN  = tf.contrib.layers.batch_norm( Out, scale = True, is_training = isTrain, scope = Name )\n",
    "            return tf.nn.relu( BN )\n",
    "\n",
    "\n",
    "    def stackedLayer   ( self, Name, Lin, Fsize, poolsize,  poolstride,  inSize, outSize,  numLayer, isTrain ):\n",
    "        with tf.variable_scope(Name, reuse=True  ):\n",
    "            \n",
    "            L       = self.convLayer                (   Name+'_0' , Lin, Fsize, inSize,  outSize )\n",
    "            BN      = tf.contrib.layers.batch_norm  (   L, scale = True, \n",
    "                                                    is_training=isTrain, scope=Name+'_0' )\n",
    "            A       = tf.nn.relu                    (   BN )\n",
    "\n",
    "            for i in range ( 1, numLayer ):\n",
    "                L   = self.convLayer                (   Name+ '_' +str(i), A, Fsize, outSize, outSize )\n",
    "                BN  = tf.contrib.layers.batch_norm  (   L, scale = True, \n",
    "                                                    is_training = isTrain, scope=Name+'_'+str(i) )\n",
    "\n",
    "                A   = tf.nn.relu                    (   BN )\n",
    "        \n",
    "            Mlast, Vlast    = tf.nn.moments( BN, [ 0,1,2,3] )  \n",
    "            Lout            = tf.nn.max_pool    (  A,   [1,poolsize,poolsize,1], [1,poolstride,poolstride,1], 'VALID' )\n",
    "            return Lout, Mlast, Vlast\n",
    "\n",
    "\n",
    "    def convLayer           ( self, Name, Lin, Fsize,Channel, Osize ):\n",
    "        with tf.variable_scope(Name, reuse=True  ):\n",
    "\n",
    "            # BHWC        \n",
    "            W   = tf.get_variable   ( Name, [Fsize,Fsize,Channel,Osize], initializer = tf.contrib.layers.xavier_initializer() )\n",
    "            L   = tf.nn.conv2d      ( Lin, W,   [1,1,1,1], 'SAME' )\n",
    "            return L\n",
    "\n",
    "\n",
    "    def normalize_input     ( self, X, M, V ):\n",
    "        return ( X - M ) / tf.sqrt ( V )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no need tensorflow\n",
    "class exRep:\n",
    "\n",
    "    def __init__( self, M, width, height ) :\n",
    "        self.M          = M\n",
    "        self.W          = width\n",
    "        self.H          = height\n",
    "\n",
    "        self.curS       = list()    # listof Matrix\n",
    "        self.curA       = list()    # listof lenth 3 onehot vector\n",
    "        self.curR       = list()    # listof Scalar\n",
    "        self.nxtS       = list()    # listof Matrix\n",
    "\n",
    "        # No Terminal State\n",
    "\n",
    "\n",
    "    def remember ( self, curS, curA, curR, nxtS ):\n",
    "\n",
    "        # remember current experience\n",
    "        self.curS.append   ( curS )\n",
    "        self.curA.append   ( curA )\n",
    "        self.curR.append   ( round( curR, 4)  )\n",
    "        self.nxtS.append    ( nxtS )\n",
    "\n",
    "        # delete oldest experience\n",
    "        if( len( self.curS ) > self.M ):\n",
    "            del self.curS[0]\n",
    "            del self.curA[0]\n",
    "            del self.curR[0]\n",
    "            del self.nxtS[0]\n",
    "\n",
    "\n",
    "    def get_Batch   ( self, sessT, QA_Tuple, state_PH, isTrain_PH, Beta, numActions, Gamma ):\n",
    "\n",
    "        curSs   = np.zeros( (Beta, self.H, self.W ) )\n",
    "        curAs   = np.zeros( (Beta, numActions ) )\n",
    "        Targets = np.zeros( (Beta, numActions ) )\n",
    "\n",
    "        # get batchsize Beta random index from Memory Size List\n",
    "        rIdxs   = random.sample( range( len( self.curS ) ), Beta )\n",
    "\n",
    "\n",
    "        for k in range ( Beta ):\n",
    "\n",
    "            input_kth   = self.curS[ rIdxs[k] ]\n",
    "            action_kth  = self.curA[ rIdxs[k] ]\n",
    "\n",
    "            QAValues    = sessT.run( QA_Tuple, feed_dict={ state_PH:self.nxtS[rIdxs[k]].reshape(1,self.H,self.W),isTrain_PH:False } )\n",
    "            nxtQs       = QAValues[0]\n",
    "\n",
    "            target_kth  = np.zeros( numActions)\n",
    "            target_kth[ np.argmax(action_kth)] = self.curR[ rIdxs[k] ]  + Gamma * nxtQs[0][np.argmax(nxtQs[0])]\n",
    "\n",
    "            curSs[k]    = input_kth\n",
    "            curAs[k]    = action_kth\n",
    "            Targets[k]  = target_kth\n",
    "    \n",
    "        return curSs, curAs, Targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_config = tf.ConfigProto()  \n",
    "gpu_config.gpu_options.allow_growth = True # only use required resource(memory)\n",
    "gpu_config.gpu_options.per_process_gpu_memory_fraction = 0.5 # restrict to 50%\n",
    "\n",
    "\n",
    "class trainModel:\n",
    "\n",
    "    def __init__   ( self,  epsilon_init, epsilon_min, maxiter, Beta, B,C,  learning_rate, P  ):\n",
    "\n",
    "        self.DataX          = list() \n",
    "        self.DataY          = list()\n",
    "\n",
    "        self.epsilon        = epsilon_init \n",
    "        self.epsilon_min    = epsilon_min\n",
    "\n",
    "        self.maxiter        = maxiter\n",
    "        self.Beta           = Beta\n",
    "        self.learning_rate  = learning_rate \n",
    "        self.P              = P \n",
    "        self.B              = B\n",
    "        self.C              = C\n",
    "\n",
    "    def set_Data    ( self, DataX, DataY ):\n",
    "        self.DataX = DataX\n",
    "        self.DataY = DataY\n",
    "\n",
    "        print('X Data:  Comp#, Days# ', len( self.DataX ), len( self.DataX[0] ))\n",
    "        print('Y Data:  Comp#, Days# ', len( self.DataY ), len( self.DataY[0] ))\n",
    "\n",
    "    def trainModel ( self, H,W, FSize, PSize, PStride, NumAction, M, Gamma  ):\n",
    "\n",
    "        # place holder\n",
    "        state       = tf.placeholder ( tf.float32, [None,H,W] )\n",
    "        isTrain     = tf.placeholder ( tf.bool, [] )\n",
    "\n",
    "        Action      = tf.placeholder ( tf.float32,  [ None,NumAction ] )\n",
    "        Target      = tf.placeholder ( tf.float32,[ None,NumAction ] )\n",
    "\n",
    "        # construct Graph\n",
    "        C           = ConstructCNN( H,W, FSize, PSize, PStride, NumAction )\n",
    "        rho_eta     = C.QValue    ( state, isTrain  )\n",
    "        Loss_Tuple  = C.optimize_Q( rho_eta[0], Action, Target, self.Beta, self.learning_rate )\n",
    "\n",
    "        sess        = tf.Session ( config = gpu_config )    # maintains network parameter theta\n",
    "        sessT       = tf.Session ( config = gpu_config )    # maintains target networ parameter theta^*\n",
    "        sess.run ( tf.global_variables_initializer () )\n",
    "\n",
    "        # saver\n",
    "        saver       = tf.train.Saver( max_to_keep = 20 )\n",
    "\n",
    "        # copy inital\n",
    "        saver.save      ( sess, '/DeepQ/' )\n",
    "        saver.restore   ( sess, '/DeepQ/' )\n",
    "        saver.restore   ( sessT, '/DeepQ/' )\n",
    "\n",
    "        # current experience\n",
    "        preS    = np.empty( (1,H,W), dtype = np.float32 )\n",
    "        preA    = np.empty( ( NumAction ), dtype = np.int32 )\n",
    "\n",
    "        curS    = np.empty( (1,H,W), dtype = np.float32 )\n",
    "        curA    = np.empty( (NumAction), dtype = np.int32 )\n",
    "        curR    = 0\n",
    "        nxtS    = np.empty( (H,W), dtype = np.float32 )\n",
    "\n",
    "        memory  = exRep( M, W, H )  # memory buffer\n",
    "        b       = 1                     # iteration counter\n",
    "\n",
    "        while True:\n",
    "\n",
    "            #1.0 get random valid index c, t\n",
    "            c       = random.randrange( 0, len( self.DataX ) )\n",
    "            t       = random.randrange( 1, len( self.DataX[c] ) -1  )\n",
    "\n",
    "            #1.1 get preS\n",
    "            preS    = self.DataX[c][t-1]\n",
    "            \n",
    "            #1.2 get preA by applying epsilon greedy policy to preS\n",
    "            if( self.randf(0,1) <= self.epsilon):\n",
    "                preA        = self.get_randaction   ( NumAction ) \n",
    "            else:                    \n",
    "                QAValues    = sess.run              ( rho_eta, feed_dict={ state: preS.reshape(1,H,W), isTrain:False } )\n",
    "                preA        = QAValues[1].reshape   ( NumAction )\n",
    "\n",
    "            #1.3 get curS\n",
    "            curS    = self.DataX[c][t]\n",
    "\n",
    "            #1.4 get curA by applying epsilon greedy policy to curS\n",
    "            if( self.randf(0,1) <= self.epsilon):\n",
    "                curA        = self.get_randaction   ( NumAction ) \n",
    "            else:                    \n",
    "                QAValues    = sess.run              ( rho_eta, feed_dict={ state: curS.reshape(1,H,W), isTrain:False } )\n",
    "                curA        = QAValues[1].reshape   ( NumAction )\n",
    "\n",
    "            #1.5 get current reward and next state\n",
    "            curR    = self.get_reward( preA, curA, self.DataY[c][t], self.P )\n",
    "            nxtS    = self.DataX[c][t+1]\n",
    "\n",
    "            #1.6 remember experience : tuple of curS, curA, curR, nxtS   \n",
    "            memory.remember( curS, curA, curR, nxtS )\n",
    "\n",
    "            #1.7: set epsilon                       \n",
    "            if ( self.epsilon > self.epsilon_min ):\n",
    "                self.epsilon = self.epsilon * 0.999999  \n",
    "\n",
    "            #2: update network parameter theta  every  B iteration\n",
    "            if ( len( memory.curS ) >= M ) and( b % self.B == 0 ) :\n",
    "\n",
    "                #2.1:  update Target network parameter theta^*\n",
    "                if( b % ( self.C * self.B ) == 0 )  : \n",
    "                    saver.save      ( sess, '/DeepQ/'  )\n",
    "                    saver.restore   ( sessT, '/DeepQ/' )\n",
    "\n",
    "                #2.2: sample Beta size batch from memory buffer and take gradient step with repect to network parameter theta \n",
    "                S,A,Y   = memory.get_Batch  ( sessT, rho_eta, state, isTrain,  self.Beta, NumAction, Gamma )\n",
    "                Opts    = sess.run          ( Loss_Tuple, feed_dict = { state:S, isTrain:True, Action:A, Target:Y }  )\n",
    "\n",
    "                #2.3: print Loss \n",
    "                if( b % ( 100 * self.B  ) == 0 ):\n",
    "                    print('Loss: ' ,b, Opts[0]) \n",
    "\n",
    "            #3: update iteration counter\n",
    "            b   = b + 1\n",
    "\n",
    "            #4: save model \n",
    "            if( b >= self.maxiter ):\n",
    "                saver.save( sess, \"/DeepQ/\" )\n",
    "                print('Finish! ')\n",
    "                return 0\n",
    "\n",
    "\n",
    "    def validate_Neutralized_Portfolio       ( self, DataX, DataY, sess, rho_eta, state, isTrain, NumAction, H,W  ):\n",
    "       \n",
    "        # list\n",
    "        N           = len( DataX )\n",
    "        Days        = len( DataX[0] )\n",
    "        curA        = np.zeros(( N, NumAction ))\n",
    "\n",
    "        # alpha\n",
    "        preAlpha_n  = np.zeros( N )\n",
    "        curAlpha_n  = np.zeros( N )\n",
    "        posChange   = 0\n",
    "\n",
    "        # reward\n",
    "        curR        = np.zeros( N )\n",
    "        avgDailyR   = np.zeros( Days )\n",
    "\n",
    "\n",
    "        # cumulative asset:  initialize cumAsset to 1.0\n",
    "        cumAsset    = 1\n",
    "\n",
    "        for t in range ( Days - 1 ):\n",
    "    \n",
    "            for c in range ( N ):\n",
    "           \n",
    "                #1: choose action from current state \n",
    "                curS        = DataX[c][t]\n",
    "                QAValues    = sess.run  ( rho_eta, feed_dict={ state: curS.reshape(1,H,W), isTrain:False } )\n",
    "                curA[c]     = np.round  ( QAValues[1].reshape( ( NumAction) ) )\n",
    "            \n",
    "            # set Neutralized portfolio for day t\n",
    "            curAlpha_n  = self.get_NeutralizedPortfolio ( curA,  N  )\n",
    "\n",
    "            for c in range ( N ) :\n",
    "\n",
    "                #1: get daily reward sum \n",
    "                curR[c]                     = np.round(  curAlpha_n[c] * DataY[c][t], 8)\n",
    "                avgDailyR[t]                = np.round(  avgDailyR[t] + curR[c], 8 )\n",
    "\n",
    "                #2: pos change sum\n",
    "                posChange                   = np.round(  posChange +  abs( curAlpha_n[c] - preAlpha_n[c] ), 8)\n",
    "                preAlpha_n[c]               = curAlpha_n[c]\n",
    "\n",
    "\n",
    "        # calculate cumulative return\n",
    "        for t in range( Days ):\n",
    "            cumAsset = round ( cumAsset + ( cumAsset * avgDailyR[t] * 0.01  ), 8 )\n",
    "\n",
    "        print('cumAsset ',  cumAsset)\n",
    "        return N, posChange, cumAsset, avgDailyR\n",
    "\n",
    "\n",
    "    def validate_TopBottomK_Portfolio       ( self, DataX, DataY, sess, rho_eta, state, isTrain, NumAction, H,W, K  ):\n",
    "\n",
    "        # list\n",
    "        N           = len( DataX )\n",
    "        Days        = len( DataX[0] )\n",
    "\n",
    "        # alpha\n",
    "        preAlpha_s  = np.zeros( N )\n",
    "        curAlpha_s  = np.zeros( N )\n",
    "        posChange   = 0\n",
    "\n",
    "        # reward\n",
    "        curR        = np.zeros( N )\n",
    "        avgDailyR   = np.zeros( Days )\n",
    "      \n",
    "        # cumulative asset: initialize curAsset to 1.0\n",
    "        cumAsset    = 1\n",
    "\n",
    "        # action value for Signals and Threshold for Top/Bottom K \n",
    "        curActValue = np.zeros( (N, NumAction ) )\n",
    "        LongSignals = np.zeros( N )\n",
    "\n",
    "        UprTH       = 0\n",
    "        LwrTH       = 0\n",
    "\n",
    "        for t in range ( Days - 1 ):\n",
    "\n",
    "            for c in range ( N ):\n",
    "           \n",
    "                #1: choose action from current state \n",
    "                curS            = DataX[c][t]\n",
    "                QAValues        = sess.run  ( rho_eta, feed_dict={ state: curS.reshape(1,H,W), isTrain:False } )\n",
    "                curActValue[c]  = np.round  ( QAValues[0].reshape( ( NumAction) ), 4 )\n",
    "                LongSignals[c]  = curActValue[c][0] - curActValue[c][2]\n",
    "\n",
    "            # set Top/Bottom portfolio for day t\n",
    "            UprTH, LwrTH        = self.givenLongSignals_getKTH  ( LongSignals, K, t  ) \n",
    "            curAlpha_s          = self.get_TopBottomPortfolio   ( UprTH, LwrTH, LongSignals, N )       \n",
    "\n",
    "            for c in range ( N ):\n",
    "\n",
    "                #1: get daily reward sum\n",
    "                curR[c]                     = np.round(  curAlpha_s[c] * DataY[c][t], 8)\n",
    "                avgDailyR[t]                = np.round(  avgDailyR[t] + curR[c], 8 )\n",
    "\n",
    "                #2: pos change sum\n",
    "                posChange                   = np.round(  posChange +  abs( curAlpha_s[c] - preAlpha_s[c] ), 8)\n",
    "                preAlpha_s[c]               = curAlpha_s[c]\n",
    "\n",
    "\n",
    "        # calculate cumulative return\n",
    "        for t in range( Days ):\n",
    "            cumAsset = round (cumAsset + ( cumAsset * avgDailyR[t] * 0.01  ), 8 )\n",
    "\n",
    "        print('cumAsset ',  cumAsset)\n",
    "        return N, posChange, cumAsset\n",
    "\n",
    "\n",
    "    def TestModel_ConstructGraph    ( self, H,W, FSize, PSize, PStride,  NumAction  ):\n",
    "\n",
    "        # place holder\n",
    "        state       = tf.placeholder ( tf.float32, [None,H,W] )\n",
    "        isTrain     = tf.placeholder ( tf.bool, [] )\n",
    "\n",
    "        #print tf.shape( isTrain)\n",
    "        #print(tf.__version__)\n",
    "\n",
    "        # construct Graph\n",
    "        C           = ConstructCNN( H,W, FSize, PSize, PStride, NumAction )\n",
    "        rho_eta     = C.QValue    ( state, isTrain  )\n",
    "\n",
    "        sess        = tf.Session ( config = gpu_config )\n",
    "        saver       = tf.train.Saver()\n",
    "\n",
    "        return sess, saver, state, isTrain, rho_eta\n",
    "\n",
    "    def Test_TopBottomK_Portfolio   ( self, sess, saver, state, isTrain, rho_eta,  H,W, NumAction, TopK  ):\n",
    "\n",
    "        saver.restore( sess, '/DeepQ/' )\n",
    "        Outcome     = self.validate_TopBottomK_Portfolio (  self.DataX, self.DataY, sess, rho_eta, state, isTrain, NumAction, H,W, TopK  )\n",
    "\n",
    "        print('NumComp#: ',  Outcome[0],  'Transactions: ', Outcome[1]/2, 'cumulative asset',Outcome[2])\n",
    "        self.writeResult_daily( 'TestResult.txt', Outcome,  len ( self.DataX[0] ) -1  )\n",
    "\n",
    "\n",
    "    def Test_Neutralized_Portfolio  ( self, sess, saver, state, isTrain,  rho_eta,  H,W, NumAction  ):\n",
    "\n",
    "        saver.restore( sess, '/DeepQ/' )\n",
    "        Outcome      = self.validate_Neutralized_Portfolio (  self.DataX, self.DataY, sess, rho_eta, state, isTrain, NumAction, H,W  )\n",
    "\n",
    "        print('NumComp#: ',  Outcome[0],  'Transactions: ', Outcome[1]/2, 'cumulative asset',Outcome[2])\n",
    "        self.writeResult_daily ( 'TestResult.txt', Outcome, len( self.DataX[0] ) -1  )\n",
    "        self.write_avgDailyR('avgDailyR.txt', Outcome[3])\n",
    "\n",
    " \n",
    "    def get_NeutralizedPortfolio         ( self, curA, N ):         \n",
    "        \n",
    "        alpha       = np.zeros( N )\n",
    "        avg         = 0\n",
    "        \n",
    "        # get average\n",
    "        for c in range ( N ):\n",
    "            alpha[c]    = 1 - np.argmax( curA[c] )\n",
    "            avg         = avg + alpha[c]\n",
    "            \n",
    "        avg     = np.round( avg / N, 4 )\n",
    "\n",
    "        #set alpha\n",
    "        sum_a       = 0\n",
    "        for c in range ( N ):\n",
    "            alpha[c]= np.round( alpha[c] - avg, 4 )\n",
    "            sum_a   = np.round( sum_a + abs(alpha[c]), 4 )\n",
    "\n",
    "        #set alpha\n",
    "        if sum_a == 0 :\n",
    "            return alpha\n",
    "\n",
    "        for c in range ( N ):\n",
    "            alpha[c] =np.round(  alpha[c] / sum_a, 8 )\n",
    "\n",
    "        alpha[0] = 1 - np.argmax( curA[0] )\n",
    "\n",
    "        return alpha\n",
    "\n",
    "\n",
    "    def givenLongSignals_getKTH       ( self, LongSignals, K, t  ):\n",
    "        \n",
    "        Num         =  int( len(LongSignals) * K)\n",
    "        SortedLongS =  np.sort( LongSignals )\n",
    "\n",
    "        return SortedLongS[len(LongSignals) - Num], SortedLongS[Num-1]\n",
    "\n",
    "\n",
    "    def get_TopBottomPortfolio              ( self, UprTH, LwrTH, LongSignals, N ):\n",
    "\n",
    "        alpha   = np.zeros( N )\n",
    "        sum_a   = 0\n",
    "\n",
    "        for c in range ( N ):\n",
    "            if LongSignals[c] >= UprTH:\n",
    "                alpha[c] = 1\n",
    "                sum_a = sum_a + 1\n",
    "            elif LongSignals[c] <= LwrTH:\n",
    "                alpha[c] = -1\n",
    "                sum_a = sum_a+1\n",
    "            else:\n",
    "                alpha[c] = 0\n",
    "\n",
    "        if sum_a == 0: \n",
    "            return alpha\n",
    "\n",
    "        for c in range ( N ) :\n",
    "            alpha[c] = np.round( alpha[c] / float(sum_a), 8 )\n",
    "\n",
    "        return alpha\n",
    "        \n",
    "\n",
    "    def randf           ( self,  s, e):\n",
    "        return (float(random.randrange(0, (e - s) * 9999)) / 10000) + s;\n",
    "\n",
    "    def get_randaction  ( self,  numofaction ) :\n",
    "        actvec      =  np.zeros( (numofaction), dtype = np.int32 )\n",
    "        idx         =  random.randrange(0,numofaction)\n",
    "        actvec[idx] = 1\n",
    "        return actvec\n",
    "\n",
    "\n",
    "    def get_reward    ( self, preA, curA, inputY, P ):\n",
    "        \n",
    "        # 1,0,-1 is assined to pre_act, cur_act \n",
    "        # for action long, neutral, short respectively\n",
    "        pre_act = 1- np.argmax( preA ) \n",
    "        cur_act = 1- np.argmax( curA ) \n",
    "\n",
    "        return  (cur_act * inputY) - P * abs( cur_act - pre_act ) \n",
    "\n",
    "\n",
    "    def writeResult_daily    ( self,  filename,  outcome, numDays ):\n",
    "        f = open( filename, 'a' )\n",
    "\n",
    "        f.write( 'Comp#,'       + str( outcome[0]) + ',' )\n",
    "        f.write( 'Days#'        + str( numDays-1 ) + ',' )\n",
    "        f.write( 'TR#,'         + str( round( outcome[1]/2, 4) ) + ',' )\n",
    "        f.write( 'FinalAsset,'  + str( round( outcome[2], 4 )) )\n",
    "        \n",
    "        f.write(\"\\n\")\n",
    "        f.close()\n",
    "\n",
    "    def write_avgDailyR    ( self,  filename,  outcome):\n",
    "        f = open( filename, 'w' )\n",
    "\n",
    "        for dailyR in outcome:\n",
    "            f.write(str(round(dailyR,8)))\n",
    "            f.write(\"\\n\")\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FSize           = 5     \n",
    "PSize           = 2\n",
    "PStride         = 2\n",
    "NumAction       = 3\n",
    "\n",
    "\n",
    "\n",
    "# hyper parameters described in the paper \n",
    "#################################################################################\n",
    "maxiter         = 5000000       # maxmimum iteration number         \n",
    "learning_rate   = 0.00001       # learning rate\n",
    "epsilon_min     = 0.1           # minimum epsilon\n",
    "\n",
    "W               = 32            # input matrix size\n",
    "M               = 1000          # memory buffer capacity\n",
    "B               = 10            # parameter theta  update interval               \n",
    "C               = 1000          # parameter theta^* update interval ( TargetQ )\n",
    "Gamma           = 0.99          # discount factor\n",
    "P               = 0.04             # transaction panalty while training.  0.05 (%) for training, 0 for testing\n",
    "Beta            = 32            # batch size\n",
    "#################################################################################\n",
    "\n",
    "# initialize\n",
    "DRead           = DataReaderRL()\n",
    "Model           = trainModel( 1.0, epsilon_min, maxiter, Beta, B , C, learning_rate, P  )\n",
    "\n",
    "\n",
    "\n",
    "######## Test Model ###########\n",
    "'''\n",
    "# folder list for testing \n",
    "folderlist                          =  DRead.get_filelist(  'Sample_Testing/')\n",
    "sess,saver, state, isTrain, rho_eta = Model.TestModel_ConstructGraph( W,W,FSize,PSize,PStride,NumAction )\n",
    "\n",
    "for i in range ( 0, len( folderlist) ):\n",
    "\n",
    "    print(folderlist[i])\n",
    "   \n",
    "    filepathX       =   folderlist[i] + 'inputX.txt'\n",
    "    filepathY       =   folderlist[i] + 'inputY.txt' \n",
    "\n",
    "    XData           =   DRead.readRaw_generate_X( filepathX, W, W )\n",
    "    YData           =   DRead.readRaw_generate_Y( filepathY, len(XData), len(XData[0]) )   \n",
    "\n",
    "    Model.set_Data                          ( XData, YData )\n",
    "    Model.Test_Neutralized_Portfolio        ( sess, saver, state, isTrain, rho_eta, W, W, NumAction )\n",
    "    Model.Test_TopBottomK_Portfolio         ( sess, saver, state, isTrain, rho_eta, W, W, NumAction,  0.2 )\n",
    "'''\n",
    "\n",
    "sess,saver, state, isTrain, rho_eta = Model.TestModel_ConstructGraph( W,W,FSize,PSize,PStride,NumAction )\n",
    "\n",
    "filepathX       =   'input_test_X.txt'\n",
    "filepathY       =   'input_test_Y.txt' \n",
    "\n",
    "XData           =   DRead.readRaw_generate_X( filepathX, W, W )\n",
    "YData           =   DRead.readRaw_generate_Y( filepathY, len(XData), len(XData[0]) )   \n",
    "\n",
    "Model.set_Data                          ( XData, YData )\n",
    "Model.Test_Neutralized_Portfolio        ( sess, saver, state, isTrain, rho_eta, W, W, NumAction )\n",
    "Model.Test_TopBottomK_Portfolio         ( sess, saver, state, isTrain, rho_eta, W, W, NumAction,  0.2 )\n",
    "\n",
    "###################################\n",
    "\n",
    "########## Train Model ############\n",
    "\n",
    "'''\n",
    "# folder path for training\n",
    "\n",
    "filepathX       =   'input_train_X.txt'\n",
    "filepathY       =   'input_train_Y.txt'\n",
    "\n",
    "\n",
    "XData           = DRead.readRaw_generate_X( filepathX, W, W )                       # input chart\n",
    "YData           = DRead.readRaw_generate_Y( filepathY, len(XData), len(XData[0]) )  # L_c^t  \n",
    "Model.set_Data      ( XData, YData)\n",
    "Model.trainModel    ( W,W, FSize, PSize, PStride, NumAction, M, Gamma )\n",
    "'''\n",
    "####################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
